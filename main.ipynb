{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential, clone_model\n",
    "from keras.layers import Conv2D, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque\n",
    "from gymnasium.wrappers import FrameStack\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"ALE/Tetris-v5\")\n",
    "env.reset()\n",
    "done = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = env.observation_space.shape\n",
    "action_size = env.action_space.n\n",
    "memory = deque(maxlen=2000)\n",
    "batch_size = 64\n",
    "gamma = 0.95\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "target_update_counter = 0\n",
    "update_target_every = 5\n",
    "episodes = 100\n",
    "max_steps_per_episode = 5000\n",
    "learning_rate = 0.001\n",
    "update_target_every = 10  # Update target DQN every 10 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, action_size):\n",
    "    model = Sequential([ # Each person should change the amount of Conv2D/Dense layers, as well as the filter amount and kernel_size/strides\n",
    "        Conv2D(32, kernel_size=(8, 8), strides=(4, 4), activation='relu', input_shape=input_shape),\n",
    "        Conv2D(64, kernel_size=(4, 4), strides=(2, 2), activation='relu'),\n",
    "        Conv2D(64, kernel_size=(3, 3), strides=(1, 1), activation='relu'),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dense(action_size, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bsilver3\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    }
   ],
   "source": [
    "dqn = build_model(state_size, action_size)\n",
    "# Define target DQN model\n",
    "target_dqn = clone_model(dqn)\n",
    "target_dqn.set_weights(dqn.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for episode in tqdm(range(episodes), desc='Episode Progress'):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    for step in range(max_steps_per_episode):\n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = env.action_space.sample()  # Exploration\n",
    "        else:\n",
    "            q_values = dqn.predict(np.array([state]), verbose=None)[0]\n",
    "            action = np.argmax(q_values)  # Exploitation\n",
    "\n",
    "        # Ensure action is within bounds\n",
    "        action = np.clip(action, 0, action_size - 1)\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        # Perform batch update every few steps\n",
    "        if len(memory) >= batch_size:\n",
    "            batch = random.sample(memory, batch_size)\n",
    "            states = np.array([sample[0] for sample in batch])\n",
    "            actions = np.array([sample[1] for sample in batch])\n",
    "            rewards = np.array([sample[2] for sample in batch])\n",
    "            next_states = np.array([sample[3] for sample in batch])\n",
    "            dones = np.array([sample[4] for sample in batch])\n",
    "\n",
    "            # Calculate target Q-values using target DQN\n",
    "            target_q_values = target_dqn.predict(next_states, verbose=0)\n",
    "            target_actions = np.argmax(target_q_values, axis=1)  # Get the index of the action with maximum Q-value\n",
    "            target_values = dqn.predict(next_states, verbose=0)  # Get Q-values for the next states from the main DQN\n",
    "            targets = rewards + (1 - dones) * gamma * target_values[np.arange(batch_size), target_actions]\n",
    "\n",
    "            # Ensure targets have the appropriate shape for the loss calculation\n",
    "            targets = targets.reshape(-1, 1)  # Reshape to (batch_size, 1) if necessary\n",
    "\n",
    "            # Update Q-values using the target Q-values\n",
    "            dqn.fit(states, targets, epochs=1, verbose=0)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Decay exploration rate\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    # Update target DQN weights periodically\n",
    "    if episode % update_target_every == 0:\n",
    "        target_dqn.set_weights(dqn.get_weights())\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('brennan.weights.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
