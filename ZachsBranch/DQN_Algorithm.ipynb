{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c038f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import keras\n",
    "from keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
    "from gymnasium.wrappers import FrameStack\n",
    "from gymnasium.experimental.wrappers import GrayscaleObservationV0\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from IPython.display import clear_output\n",
    "%config NotebookApp.iopub_msg_rate_limit=10000\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43be4357",
   "metadata": {},
   "outputs": [],
   "source": [
    "totalEpisodesTrained = pd.read_csv('totalEpisodesTrained.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "173cdf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "totalEpisodesTrained = totalEpisodesTrained[['episodes', 'time']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15fa766e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episodes</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1886</td>\n",
       "      <td>140753.993567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episodes           time\n",
       "0      1886  140753.993567"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalEpisodesTrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe108190",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"ALE/Tetris-v5\") # Human render mode slows things down A LOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21502e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = GrayscaleObservationV0(env) # Remove RGB channels (make gray) in order to decrease amount of data to process\n",
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2e644cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zache\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3444837047, 2669555309)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42 # Allows us to repeat the same patterns of game play\n",
    "env = FrameStack(env, 4) # Get 4 frames from game at a time\n",
    "frames, width, height = env.observation_space.shape\n",
    "env.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cafee4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 210 160\n"
     ]
    }
   ],
   "source": [
    "print(frames,width,height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bca17ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This network learns an approximation of the Q-table, which is a mapping between the states and actions that an agent will take. \n",
    "# For every state we'll have FIVE actions that can be taken (NOOP, Up, Down, Left, Right). \n",
    "# The environment provides the state, and the action is chosen by selecting the largest of the five Q-values predicted in the output layer of the CNN.\n",
    "\n",
    "num_actions = 5\n",
    "\n",
    "# three convolution and three dense layers\n",
    "def create_CNN():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(32, (8, 8), strides=(4, 4), padding='same', activation='relu', input_shape=(4, 210, 160)))\n",
    "    model.add(Conv2D(64, (4, 4), strides=(2, 2), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(num_actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "517c33db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zache\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# The first neural net makes the predictions for Q-values, which are used to take an action.\n",
    "cnn1 = create_CNN()\n",
    "if os.path.exists('cnn1v2.weights.h5'):\n",
    "    cnn1.load_weights('cnn1v2.weights.h5')\n",
    "\n",
    "# A second cnn is used to predict future rewards. The weights of the second cnn get updated every 10000 steps.\n",
    "cnn2 = create_CNN()\n",
    "if os.path.exists('cnn2v2.weights.h5'):\n",
    "    cnn2.load_weights('cnn2v2.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a18a700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamaters DQN Algorithm\n",
    "\n",
    "gamma = 0.99  # Discount factor in Bellman's equuation\n",
    "epsilon = 1  # Epsilon greedy parameter for Q learning algorithm\n",
    "max_steps_per_episode = 100000 #Deepmind trained for \"a total of 50 million frames (~38 days of game play)\"\n",
    "max_episodes = 10000  # Number of episodes you let the AI train. Keep above 1!\n",
    "epsilon_min = 0.1  # Smallest epsilon value possible\n",
    "epsilon_max = 1.0  # Largest epsilon value possible\n",
    "epsilon_interval = (epsilon_max - epsilon_min)  # Rate we reduce chance of random action being taken (eventually, we don't want to take many random actions)\n",
    "\n",
    "# Somre more important variables\n",
    "batch_size = 32  # Size of sample taken from \"replay buffer\"\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "\n",
    "# Experience replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "episode_reward_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "# Number of frames to take random action and observe output\n",
    "epsilon_random_frames = 50000\n",
    "# Number of frames for exploration\n",
    "epsilon_greedy_frames = 1000000.0\n",
    "# Maximum replay length\n",
    "# Note: A Deepmind paper suggests 1000000, however this can cause memory issues\n",
    "max_memory_length = 100000\n",
    "# Train the model after 4 actions\n",
    "update_after_actions = 4\n",
    "# How often to update cnn2\n",
    "update_cnn2 = 10000\n",
    "# Using huber loss to check for convergance of Qs\n",
    "loss_function = keras.losses.Huber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cbddf0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 108\u001b[0m\n\u001b[0;32m    105\u001b[0m     percent_scored_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m((episodes_scored\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(episode_scores))\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    107\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m--> 108\u001b[0m clear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(episode_count \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(max_episodes))\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent episode score: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(current_episode_score))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\display_functions.py:386\u001b[0m, in \u001b[0;36mclear_output\u001b[1;34m(wait)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minteractiveshell\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InteractiveShell\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m InteractiveShell\u001b[38;5;241m.\u001b[39minitialized():\n\u001b[1;32m--> 386\u001b[0m     InteractiveShell\u001b[38;5;241m.\u001b[39minstance()\u001b[38;5;241m.\u001b[39mdisplay_pub\u001b[38;5;241m.\u001b[39mclear_output(wait)\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[2K\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py:155\u001b[0m, in \u001b[0;36mZMQDisplayPublisher.clear_output\u001b[1;34m(self, wait)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39msend(\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_socket,\n\u001b[0;32m    157\u001b[0m     msg,\n\u001b[0;32m    158\u001b[0m     ident\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtopic,\n\u001b[0;32m    159\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\jupyter_client\\session.py:859\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, stream, msg_or_type, content, parent, ident, buffers, track, header, metadata)\u001b[0m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m stream:\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;66;03m# use dummy tracker, which will be done immediately\u001b[39;00m\n\u001b[0;32m    858\u001b[0m     tracker \u001b[38;5;241m=\u001b[39m DONE\n\u001b[1;32m--> 859\u001b[0m     stream\u001b[38;5;241m.\u001b[39msend_multipart(to_send, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    861\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[0;32m    862\u001b[0m     pprint\u001b[38;5;241m.\u001b[39mpprint(msg)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\iostream.py:330\u001b[0m, in \u001b[0;36mBackgroundSocket.send_multipart\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Schedule send in IO thread\"\"\"\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mio_thread \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 330\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mio_thread\u001b[38;5;241m.\u001b[39msend_multipart(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\iostream.py:260\u001b[0m, in \u001b[0;36mIOPubThread.send_multipart\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_multipart\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    256\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"send_multipart schedules actual zmq send in my thread.\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \n\u001b[0;32m    258\u001b[0m \u001b[38;5;124;03m    If my thread isn't running (e.g. forked process), send immediately.\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschedule(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_really_send(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\iostream.py:251\u001b[0m, in \u001b[0;36mIOPubThread.schedule\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_events\u001b[38;5;241m.\u001b[39mappend(f)\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;66;03m# wake event thread (message content is ignored)\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_pipe\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    253\u001b[0m     f()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\zmq\\sugar\\socket.py:618\u001b[0m, in \u001b[0;36mSocket.send\u001b[1;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[0;32m    611\u001b[0m         data \u001b[38;5;241m=\u001b[39m zmq\u001b[38;5;241m.\u001b[39mFrame(\n\u001b[0;32m    612\u001b[0m             data,\n\u001b[0;32m    613\u001b[0m             track\u001b[38;5;241m=\u001b[39mtrack,\n\u001b[0;32m    614\u001b[0m             copy\u001b[38;5;241m=\u001b[39mcopy \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    615\u001b[0m             copy_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy_threshold,\n\u001b[0;32m    616\u001b[0m         )\n\u001b[0;32m    617\u001b[0m     data\u001b[38;5;241m.\u001b[39mgroup \u001b[38;5;241m=\u001b[39m group\n\u001b[1;32m--> 618\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msend(data, flags\u001b[38;5;241m=\u001b[39mflags, copy\u001b[38;5;241m=\u001b[39mcopy, track\u001b[38;5;241m=\u001b[39mtrack)\n",
      "File \u001b[1;32mzmq\\backend\\cython\\socket.pyx:740\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mzmq\\backend\\cython\\socket.pyx:787\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mzmq\\backend\\cython\\socket.pyx:244\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\zmq\\backend\\cython\\checkrc.pxd:13\u001b[0m, in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# DQN Algorithm\n",
    "start_time = time.time()\n",
    "episode_scores = []\n",
    "current_episode_score = 0\n",
    "while True:\n",
    "    observation, _ = env.reset()\n",
    "    state = np.array(observation)\n",
    "    episode_reward = 0\n",
    "    episode_start_time = time.time()\n",
    "    \n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        frame_count += 1\n",
    "        # Use epsilon-greedy policy to explore or exploit\n",
    "            # If current frame_count is less than 50,000 or epsilon is greater than a random number between 0 and 1\n",
    "#         print(\"Frame Count: \", frame_count)\n",
    "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            # Take random action\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            # Predict Q-values from environment state\n",
    "            state_tensor = keras.ops.convert_to_tensor(state) # Turn state (frame in game) into a Tensor Object (think matrix)\n",
    "            state_tensor = keras.ops.expand_dims(state_tensor, 0) # Add to the current batch\n",
    "            action_probs = cnn1(state_tensor, training=False)\n",
    "#             print(action_probs)\n",
    "            # Take best action\n",
    "            action = keras.ops.argmax(action_probs[0]).numpy()\n",
    "        \n",
    "        # Decrease probability of taking random action\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "        \n",
    "        # Take action in environment\n",
    "#         print(\"Action: \", action)\n",
    "        state_next, reward, done, _, _ = env.step(action)\n",
    "        state_next = np.array(state_next)\n",
    "        \n",
    "        # Sum rewards across entire episode\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Save actions and states in \"replay buffer\"\n",
    "        action_history.append(action)\n",
    "        state_history.append(state)\n",
    "        state_next_history.append(state_next)\n",
    "        done_history.append(done)\n",
    "        current_episode_score += reward\n",
    "        rewards_history.append(reward)\n",
    "        state = state_next\n",
    "        \n",
    "        # Update every fourth frame AND once batch size is greater than 32\n",
    "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "            # Get indices of samples for replay buffers\n",
    "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
    "\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            state_sample = np.array([state_history[i] for i in indices])\n",
    "            state_next_sample = np.array([state_next_history[i] for i in indices])\n",
    "            rewards_sample = [rewards_history[i] for i in indices]\n",
    "            action_sample = [action_history[i] for i in indices]\n",
    "            done_sample = keras.ops.convert_to_tensor(\n",
    "                [float(done_history[i]) for i in indices]\n",
    "            )\n",
    "\n",
    "            # Build the updated Q-values for the sampled future states using cnn2\n",
    "            future_rewards = cnn2.predict(state_next_sample)\n",
    "            # Q value = reward + discount factor * expected future reward (Bellman's Equation)\n",
    "            updated_q_values = rewards_sample + gamma * keras.ops.amax(future_rewards, axis=1)\n",
    "\n",
    "            # If final frame set the last value to -1\n",
    "            updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
    "\n",
    "            # Create a 'mask' (matrix of 0s and 1s) so we only calculate loss on the updated Q-values\n",
    "            masks = keras.ops.one_hot(action_sample, num_actions)\n",
    "            \n",
    "            # Train the cnn1 using updated Q-values\n",
    "            with tf.GradientTape() as tape:\n",
    "                q_values = cnn1(state_sample)\n",
    "\n",
    "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                q_action = keras.ops.sum(keras.ops.multiply(q_values, masks), axis=1)\n",
    "                # Calculate loss between new Q-value and old Q-value\n",
    "                loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "            # Backpropagation - how cnn1 is updated\n",
    "            grads = tape.gradient(loss, cnn1.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, cnn1.trainable_variables))\n",
    "        \n",
    "        # Time to update cnn2?\n",
    "        if frame_count % update_cnn2 == 0:\n",
    "            # update cnn2 with new weights\n",
    "            cnn2.set_weights(cnn1.get_weights())\n",
    "            # Log details\n",
    "#             template = \"running reward: {:.2f} at episode {}, frame count {}\"\n",
    "            #print(template.format(running_reward, episode_count, frame_count))\n",
    "\n",
    "        # Limit the state and reward history\n",
    "        if len(rewards_history) > max_memory_length:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "         \n",
    "        if len(episode_scores) > 0:\n",
    "            episodes_scored = sum(score > 0 for score in episode_scores)\n",
    "            percent_scored_in = round((episodes_scored/len(episode_scores))*100, 2)\n",
    "        \n",
    "        time.sleep(0.001)\n",
    "        clear_output(wait=True)\n",
    "        print(\"episode \" + str(episode_count + 1) + \"/\" + str(max_episodes))\n",
    "        print(\"current episode score: \" + str(current_episode_score))\n",
    "        if len(episode_scores) > 0:\n",
    "            print(\"avg score/episode (this training session): \" + str(np.mean(episode_scores)))\n",
    "            print(\"best episode score (this training session): \" + str(max(episode_scores)))\n",
    "            print(\"% of episodes scored in: \" + str(percent_scored_in) + \"%\")\n",
    "        \n",
    "        print(\"-------------------------------------------------------------------\")\n",
    "        \n",
    "        print(\"training session time: \" + str(round((time.time() - start_time)/60,2)) + \" min\")\n",
    "        print(\"current episode time: \" + str(round((time.time() - episode_start_time),2)) + \" sec\")\n",
    "        \n",
    "        print(\"-------------------------------------------------------------------\")\n",
    "        \n",
    "        print(\"total episodes trained \" + str(totalEpisodesTrained.loc[0, \"episodes\"]))\n",
    "        print(\"total time trained: \" + str(round(totalEpisodesTrained.loc[0, \"time\"]/60,2)) + \" min\")\n",
    "\n",
    "        if done:\n",
    "            episode_scores.append(current_episode_score)\n",
    "            current_episode_score = 0\n",
    "#             print(\"DONE\")\n",
    "            break\n",
    "\n",
    "    # Update running reward to check condition for solving\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    if len(episode_reward_history) > 100:\n",
    "        del episode_reward_history[:1]\n",
    "    \n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "#     print(\"Average Reward Across All Episodes: \", running_reward)\n",
    "    episode_count += 1\n",
    "\n",
    "    if running_reward > 40:  # Condition to consider game \"learned\"\n",
    "#         print(\"Learned at episode {}!\".format(episode_count))\n",
    "        break\n",
    "\n",
    "    if (max_episodes > 0 and episode_count >= max_episodes):  # Maximum number of episodes reached\n",
    "#         print(\"Stopped at episode {}!\".format(episode_count))\n",
    "        break\n",
    "    \n",
    "    # Save the weights of each CNN\n",
    "    totalEpisodesTrained.loc[0, \"episodes\"] += 1\n",
    "    totalEpisodesTrained.loc[0, \"time\"] += (time.time() - episode_start_time)\n",
    "    totalEpisodesTrained.to_csv('totalEpisodesTrained.csv')\n",
    "    cnn1.save_weights(\"cnn1.weights.h5\")\n",
    "    cnn2.save_weights(\"cnn2.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d362b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = create_CNN()\n",
    "# model.load_weights('cnn1.weights.h5')\n",
    "# env = gym.make(\"ALE/Tetris-v5\", render_mode=\"human\")\n",
    "# env = GrayscaleObservationV0(env)\n",
    "# env = FrameStack(env, 4) # Get 4 frames from game at a time\n",
    "# frames, width, height = env.observation_space.shape\n",
    "# env.reset()\n",
    "# game_over = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc7830e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# state, reward, game_over, x, _ = env.step(0)\n",
    "# while not game_over:\n",
    "    \n",
    "#     #Predict action using the trained model\n",
    "#     q_values = model.predict(np.expand_dims(state, axis=0))\n",
    "#     action = np.argmax(q_values)\n",
    "\n",
    "#     # Take action in the environment\n",
    "#     next_state, reward, game_over, x, _ = env.step(action)\n",
    "\n",
    "#     # Update current state\n",
    "#     state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb256b50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
