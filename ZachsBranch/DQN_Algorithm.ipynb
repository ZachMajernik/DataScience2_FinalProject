{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c038f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import keras\n",
    "from keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
    "from gymnasium.wrappers import FrameStack\n",
    "from gymnasium.experimental.wrappers import GrayscaleObservationV0\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from IPython.display import clear_output\n",
    "%config NotebookApp.iopub_msg_rate_limit=10000\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43be4357",
   "metadata": {},
   "outputs": [],
   "source": [
    "totalEpisodesTrained = pd.read_csv('totalEpisodesTrained.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "173cdf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "totalEpisodesTrained = totalEpisodesTrained[['episodes', 'time']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15fa766e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episodes</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2564</td>\n",
       "      <td>193863.776234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episodes           time\n",
       "0      2564  193863.776234"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalEpisodesTrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe108190",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"ALE/Tetris-v5\") # Human render mode slows things down A LOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21502e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = GrayscaleObservationV0(env) # Remove RGB channels (make gray) in order to decrease amount of data to process\n",
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2e644cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zmajernik\\AppData\\Roaming\\Python\\Python39\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3444837047, 2669555309)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42 # Allows us to repeat the same patterns of game play\n",
    "env = FrameStack(env, 4) # Get 4 frames from game at a time\n",
    "frames, width, height = env.observation_space.shape\n",
    "env.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cafee4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 210 160\n"
     ]
    }
   ],
   "source": [
    "print(frames,width,height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bca17ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This network learns an approximation of the Q-table, which is a mapping between the states and actions that an agent will take. \n",
    "# For every state we'll have FIVE actions that can be taken (NOOP, Up, Down, Left, Right). \n",
    "# The environment provides the state, and the action is chosen by selecting the largest of the five Q-values predicted in the output layer of the CNN.\n",
    "\n",
    "num_actions = 5\n",
    "\n",
    "# three convolution and three dense layers\n",
    "def create_CNN():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(32, (8, 8), strides=(4, 4), padding='same', activation='relu', input_shape=(4, 210, 160)))\n",
    "    model.add(Conv2D(64, (4, 4), strides=(2, 2), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(num_actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "517c33db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zmajernik\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    }
   ],
   "source": [
    "# The first neural net makes the predictions for Q-values, which are used to take an action.\n",
    "cnn1 = create_CNN()\n",
    "if os.path.exists('cnn1v2.weights.h5'):\n",
    "    cnn1.load_weights('cnn1v2.weights.h5')\n",
    "\n",
    "# A second cnn is used to predict future rewards. The weights of the second cnn get updated every 10000 steps.\n",
    "cnn2 = create_CNN()\n",
    "if os.path.exists('cnn2v2.weights.h5'):\n",
    "    cnn2.load_weights('cnn2v2.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a18a700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamaters DQN Algorithm\n",
    "\n",
    "gamma = 0.99  # Discount factor in Bellman's equuation\n",
    "epsilon = 1  # Epsilon greedy parameter for Q learning algorithm\n",
    "max_steps_per_episode = 100000 #Deepmind trained for \"a total of 50 million frames (~38 days of game play)\"\n",
    "max_episodes = 10000  # Number of episodes you let the AI train. Keep above 1!\n",
    "epsilon_min = 0.1  # Smallest epsilon value possible\n",
    "epsilon_max = 1.0  # Largest epsilon value possible\n",
    "epsilon_interval = (epsilon_max - epsilon_min)  # Rate we reduce chance of random action being taken (eventually, we don't want to take many random actions)\n",
    "\n",
    "# Somre more important variables\n",
    "batch_size = 32  # Size of sample taken from \"replay buffer\"\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "\n",
    "# Experience replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "episode_reward_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "# Number of frames to take random action and observe output\n",
    "epsilon_random_frames = 50000\n",
    "# Number of frames for exploration\n",
    "epsilon_greedy_frames = 1000000.0\n",
    "# Maximum replay length\n",
    "# Note: A Deepmind paper suggests 1000000, however this can cause memory issues\n",
    "max_memory_length = 100000\n",
    "# Train the model after 4 actions\n",
    "update_after_actions = 4\n",
    "# How often to update cnn2\n",
    "update_cnn2 = 10000\n",
    "# Using huber loss to check for convergance of Qs\n",
    "loss_function = keras.losses.Huber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cbddf0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 216/10000\n",
      "current episode score: 0.0\n",
      "avg score/episode (this training session): 0.037209302325581395\n",
      "best episode score (this training session): 1.0\n",
      "% of episodes scored in: 3.72%\n",
      "-------------------------------------------------------------------\n",
      "training session time: 149.46 min\n",
      "current episode time: 5.23 sec\n",
      "-------------------------------------------------------------------\n",
      "total episodes trained 2779\n",
      "total time trained: 3380.19 min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22096\\248532296.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;31m# Build the updated Q-values for the sampled future states using cnn2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[0mfuture_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnn2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_next_sample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m             \u001b[1;31m# Q value = reward + discount factor * expected future reward (Bellman's Equation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0mupdated_q_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrewards_sample\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfuture_rewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    455\u001b[0m     ):\n\u001b[0;32m    456\u001b[0m         \u001b[1;31m# Create an iterator that yields batches of input data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m         epoch_iterator = TFEpochIterator(\n\u001b[0m\u001b[0;32m    458\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, distribute_strategy, *args, **kwargs)\u001b[0m\n\u001b[0;32m    638\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distribute_strategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistribute_strategy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 640\u001b[1;33m         \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    641\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDistributedDataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m             dataset = self._distribute_strategy.experimental_distribute_dataset(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 649\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_adapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_tf_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0menumerate_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\trainers\\data_adapters\\array_data_adapter.py\u001b[0m in \u001b[0;36mget_tf_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;31m# simultaneous shuffles can contend on a hardware level and degrade all\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;31m# performance.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[0;32m   2297\u001b[0m     \u001b[1;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2298\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmap_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2299\u001b[1;33m     return map_op._map_v2(\n\u001b[0m\u001b[0;32m   2300\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2301\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\data\\ops\\map_op.py\u001b[0m in \u001b[0;36m_map_v2\u001b[1;34m(input_dataset, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[0;32m     35\u001b[0m       warnings.warn(\"The `deterministic` argument has no effect unless the \"\n\u001b[0;32m     36\u001b[0m                     \"`num_parallel_calls` argument is specified.\")\n\u001b[1;32m---> 37\u001b[1;33m     return _MapDataset(\n\u001b[0m\u001b[0;32m     38\u001b[0m         input_dataset, map_func, preserve_cardinality=True, name=name)\n\u001b[0;32m     39\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\data\\ops\\map_op.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[0;32m    111\u001b[0m         use_legacy_function=use_legacy_function)\n\u001b[0;32m    112\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m     variant_tensor = gen_dataset_ops.map_dataset(\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mmap_dataset\u001b[1;34m(input_dataset, other_arguments, f, output_types, output_shapes, use_inter_op_parallelism, preserve_cardinality, metadata, name)\u001b[0m\n\u001b[0;32m   3527\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3528\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3529\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   3530\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MapDataset\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother_arguments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"f\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3531\u001b[0m         \u001b[1;34m\"output_types\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"output_shapes\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# DQN Algorithm\n",
    "start_time = time.time()\n",
    "episode_scores = []\n",
    "current_episode_score = 0\n",
    "while True:\n",
    "    observation, _ = env.reset()\n",
    "    state = np.array(observation)\n",
    "    episode_reward = 0\n",
    "    episode_start_time = time.time()\n",
    "    \n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        frame_count += 1\n",
    "        # Use epsilon-greedy policy to explore or exploit\n",
    "            # If current frame_count is less than 50,000 or epsilon is greater than a random number between 0 and 1\n",
    "#         print(\"Frame Count: \", frame_count)\n",
    "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            # Take random action\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            # Predict Q-values from environment state\n",
    "            state_tensor = keras.ops.convert_to_tensor(state) # Turn state (frame in game) into a Tensor Object (think matrix)\n",
    "            state_tensor = keras.ops.expand_dims(state_tensor, 0) # Add to the current batch\n",
    "            action_probs = cnn1(state_tensor, training=False)\n",
    "#             print(action_probs)\n",
    "            # Take best action\n",
    "            action = keras.ops.argmax(action_probs[0]).numpy()\n",
    "        \n",
    "        # Decrease probability of taking random action\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "        \n",
    "        # Take action in environment\n",
    "#         print(\"Action: \", action)\n",
    "        state_next, reward, done, _, _ = env.step(action)\n",
    "        state_next = np.array(state_next)\n",
    "        \n",
    "        # Sum rewards across entire episode\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Save actions and states in \"replay buffer\"\n",
    "        action_history.append(action)\n",
    "        state_history.append(state)\n",
    "        state_next_history.append(state_next)\n",
    "        done_history.append(done)\n",
    "        current_episode_score += reward\n",
    "        rewards_history.append(reward)\n",
    "        state = state_next\n",
    "        \n",
    "        # Update every fourth frame AND once batch size is greater than 32\n",
    "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "            # Get indices of samples for replay buffers\n",
    "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
    "\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            state_sample = np.array([state_history[i] for i in indices])\n",
    "            state_next_sample = np.array([state_next_history[i] for i in indices])\n",
    "            rewards_sample = [rewards_history[i] for i in indices]\n",
    "            action_sample = [action_history[i] for i in indices]\n",
    "            done_sample = keras.ops.convert_to_tensor(\n",
    "                [float(done_history[i]) for i in indices]\n",
    "            )\n",
    "\n",
    "            # Build the updated Q-values for the sampled future states using cnn2\n",
    "            future_rewards = cnn2.predict(state_next_sample)\n",
    "            # Q value = reward + discount factor * expected future reward (Bellman's Equation)\n",
    "            updated_q_values = rewards_sample + gamma * keras.ops.amax(future_rewards, axis=1)\n",
    "\n",
    "            # If final frame set the last value to -1\n",
    "            updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
    "\n",
    "            # Create a 'mask' (matrix of 0s and 1s) so we only calculate loss on the updated Q-values\n",
    "            masks = keras.ops.one_hot(action_sample, num_actions)\n",
    "            \n",
    "            # Train the cnn1 using updated Q-values\n",
    "            with tf.GradientTape() as tape:\n",
    "                q_values = cnn1(state_sample)\n",
    "\n",
    "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                q_action = keras.ops.sum(keras.ops.multiply(q_values, masks), axis=1)\n",
    "                # Calculate loss between new Q-value and old Q-value\n",
    "                loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "            # Backpropagation - how cnn1 is updated\n",
    "            grads = tape.gradient(loss, cnn1.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, cnn1.trainable_variables))\n",
    "        \n",
    "        # Time to update cnn2?\n",
    "        if frame_count % update_cnn2 == 0:\n",
    "            # update cnn2 with new weights\n",
    "            cnn2.set_weights(cnn1.get_weights())\n",
    "            # Log details\n",
    "#             template = \"running reward: {:.2f} at episode {}, frame count {}\"\n",
    "            #print(template.format(running_reward, episode_count, frame_count))\n",
    "\n",
    "        # Limit the state and reward history\n",
    "        if len(rewards_history) > max_memory_length:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "         \n",
    "        if len(episode_scores) > 0:\n",
    "            episodes_scored = sum(score > 0 for score in episode_scores)\n",
    "            percent_scored_in = round((episodes_scored/len(episode_scores))*100, 2)\n",
    "        \n",
    "        time.sleep(0.001)\n",
    "        clear_output(wait=True)\n",
    "        print(\"episode \" + str(episode_count + 1) + \"/\" + str(max_episodes))\n",
    "        print(\"current episode score: \" + str(current_episode_score))\n",
    "        if len(episode_scores) > 0:\n",
    "            print(\"avg score/episode (this training session): \" + str(np.mean(episode_scores)))\n",
    "            print(\"best episode score (this training session): \" + str(max(episode_scores)))\n",
    "            print(\"% of episodes scored in: \" + str(percent_scored_in) + \"%\")\n",
    "        \n",
    "        print(\"-------------------------------------------------------------------\")\n",
    "        \n",
    "        print(\"training session time: \" + str(round((time.time() - start_time)/60,2)) + \" min\")\n",
    "        print(\"current episode time: \" + str(round((time.time() - episode_start_time),2)) + \" sec\")\n",
    "        \n",
    "        print(\"-------------------------------------------------------------------\")\n",
    "        \n",
    "        print(\"total episodes trained \" + str(totalEpisodesTrained.loc[0, \"episodes\"]))\n",
    "        print(\"total time trained: \" + str(round(totalEpisodesTrained.loc[0, \"time\"]/60,2)) + \" min\")\n",
    "\n",
    "        if done:\n",
    "            episode_scores.append(current_episode_score)\n",
    "            current_episode_score = 0\n",
    "#             print(\"DONE\")\n",
    "            break\n",
    "\n",
    "    # Update running reward to check condition for solving\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    if len(episode_reward_history) > 100:\n",
    "        del episode_reward_history[:1]\n",
    "    \n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "#     print(\"Average Reward Across All Episodes: \", running_reward)\n",
    "    episode_count += 1\n",
    "\n",
    "    if running_reward > 40:  # Condition to consider game \"learned\"\n",
    "#         print(\"Learned at episode {}!\".format(episode_count))\n",
    "        break\n",
    "\n",
    "    if (max_episodes > 0 and episode_count >= max_episodes):  # Maximum number of episodes reached\n",
    "#         print(\"Stopped at episode {}!\".format(episode_count))\n",
    "        break\n",
    "    \n",
    "    # Save the weights of each CNN\n",
    "    totalEpisodesTrained.loc[0, \"episodes\"] += 1\n",
    "    totalEpisodesTrained.loc[0, \"time\"] += (time.time() - episode_start_time)\n",
    "    totalEpisodesTrained.to_csv('totalEpisodesTrained.csv')\n",
    "    cnn1.save_weights(\"cnn1.weights.h5\")\n",
    "    cnn2.save_weights(\"cnn2.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d362b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = create_CNN()\n",
    "# model.load_weights('cnn1.weights.h5')\n",
    "# env = gym.make(\"ALE/Tetris-v5\", render_mode=\"human\")\n",
    "# env = GrayscaleObservationV0(env)\n",
    "# env = FrameStack(env, 4) # Get 4 frames from game at a time\n",
    "# frames, width, height = env.observation_space.shape\n",
    "# env.reset()\n",
    "# game_over = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc7830e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# state, reward, game_over, x, _ = env.step(0)\n",
    "# while not game_over:\n",
    "    \n",
    "#     #Predict action using the trained model\n",
    "#     q_values = model.predict(np.expand_dims(state, axis=0))\n",
    "#     action = np.argmax(q_values)\n",
    "\n",
    "#     # Take action in the environment\n",
    "#     next_state, reward, game_over, x, _ = env.step(action)\n",
    "\n",
    "#     # Update current state\n",
    "#     state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb256b50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
